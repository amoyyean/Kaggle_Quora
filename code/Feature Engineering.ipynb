{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting new features to measure similarities\n",
    "The model_1 notebook test some simple measure of similarities. <br>\n",
    "This notebook will focus on a broad range of measures of similarity between sentences <br>\n",
    "My reference is here: https://github.com/qqgeogor/kaggle_quora_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n",
    "\n",
    "In model_1, we only substitute the special characters. Here we will add the stemming into the processing where different forms of words will be reduced to their stem format, like \"Hits\" and \"Hitting\" will all be \"Hit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import difflib\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import pickle\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# stops = set(stopwords.words(\"english\"))\n",
    "stops = [\"a\", \"and\", \"of\", \"the\", \"to\", \"on\", \"in\", \"at\", \"is\"]\n",
    "seed = 1024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_clean_no_stemming(sentence):\n",
    "    \"\"\"\n",
    "    no stemming, only stopwords\n",
    "    \"\"\"\n",
    "    s = re.sub(\"[^a-zA-Z0-9]\", \" \", str(sentence))\n",
    "    s_list = s.lower().split(\" \")\n",
    "    s_list = [w for w in s_list if w not in stops]\n",
    "    return \" \".join(s_list)\n",
    "\n",
    "def txt_clean_stemming(sentence, st=PorterStemmer()):\n",
    "    s = re.sub(\"[^a-zA-Z0-9]\", \" \", str(sentence))\n",
    "    s_list = s.lower().split(\" \")\n",
    "    s_list = [st.stem(w) for w in s_list if w not in stops]\n",
    "    return \" \".join(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # stemming\n",
    "train[\"q1_stem\"] = train.apply(lambda x: txt_clean_stemming(x[\"question1\"]),axis = 1)\n",
    "train[\"q2_stem\"] = train.apply(lambda x: txt_clean_stemming(x[\"question2\"]),axis = 1)\n",
    "\n",
    "test[\"q1_stem\"] = test.apply(lambda x: txt_clean_stemming(x[\"question1\"]),axis = 1)\n",
    "test[\"q2_stem\"] = test.apply(lambda x: txt_clean_stemming(x[\"question2\"]),axis = 1)\n",
    "\n",
    "# no stemming\n",
    "train[\"question1\"] = train.apply(lambda x: txt_clean_no_stemming(x[\"question1\"]),axis = 1)\n",
    "train[\"question2\"] = train.apply(lambda x: txt_clean_no_stemming(x[\"question2\"]),axis = 1)\n",
    "\n",
    "test[\"question1\"] = test.apply(lambda x: txt_clean_no_stemming(x[\"question1\"]),axis = 1)\n",
    "test[\"question2\"] = test.apply(lambda x: txt_clean_no_stemming(x[\"question2\"]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store data\n",
    "train.to_csv(\"../data/train_stem.csv\")\n",
    "test.to_csv(\"../data/test_stem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete train and test\n",
    "# del train\n",
    "# del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train= pd.read_csv(\"../data/train_stem.csv\")\n",
    "# test = pd.read_csv(\"../data/test_stem.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_common_ratio(text_a,text_b):\n",
    "    \"\"\"\n",
    "    ratio of overlapping: 2*M/T\n",
    "    \"\"\"\n",
    "    seq1 = str(text_a).split()\n",
    "    seq2 = str(text_b).split()\n",
    "    seqMatch = difflib.SequenceMatcher(a=seq1, b=seq2)\n",
    "    return seqMatch.ratio()\n",
    "\n",
    "def get_interaction_ratio(text_a,text_b):\n",
    "    \"\"\"\n",
    "    ratio of interactions\n",
    "    \"\"\"\n",
    "    seq1 = set(str(text_a).split())\n",
    "    seq2 = set(str(text_b).split())\n",
    "    try:\n",
    "        seqMatch = float(len(seq1.intersection(seq2)))/len(seq1.union(seq2))\n",
    "    except ZeroDivisionError:\n",
    "        seqMatch = 0.0\n",
    "    return seqMatch\n",
    "\n",
    "def get_common_bigrams_ratio(text_a,text_b):\n",
    "    seq1 = [\" \".join(i) for i in nltk.ngrams(str(text_a).split(), 2)]\n",
    "    seq2 = [\" \".join(i) for i in nltk.ngrams(str(text_b).split(), 2)]\n",
    "    seqMatch = difflib.SequenceMatcher(a=seq1, b=seq2)\n",
    "    return seqMatch.ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract similarity\n",
    "train_common_ratio = train.apply(lambda x: get_common_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_common_ratio_stem = train.apply(lambda x: get_common_ratio(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "train_interaction_ratio = train.apply(lambda x: get_interaction_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_interaction_ratio_stem = train.apply(lambda x: get_interaction_ratio(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "train_common_bigrams_ratio = train.apply(lambda x: get_common_bigrams_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_common_bigrams_ratio_stem = train.apply(lambda x: get_common_bigrams_ratio(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "\n",
    "test_common_ratio = test.apply(lambda x: get_common_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_common_ratio_stem = test.apply(lambda x: get_common_ratio(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "test_interaction_ratio = test.apply(lambda x: get_interaction_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_interaction_ratio_stem = test.apply(lambda x: get_interaction_ratio(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "test_common_bigrams_ratio = test.apply(lambda x: get_common_bigrams_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_common_bigrams_ratio_stem = test.apply(lambda x: get_common_bigrams_ratio(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sim_feat = [\"train_common_ratio\",\"train_common_ratio_stem\",\"train_interaction_ratio\",\n",
    "                  'train_interaction_ratio_stem','train_common_bigrams_ratio','train_common_bigrams_ratio_stem']\n",
    "\n",
    "for feat in train_sim_feat:\n",
    "    with open(\"../data/{}.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(globals()[feat], handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sim_feat = [\"test_common_ratio\",\"test_common_ratio_stem\",\"test_interaction_ratio\",\n",
    "                  'test_interaction_ratio_stem','test_common_bigrams_ratio','test_common_bigrams_ratio_stem']\n",
    "\n",
    "for feat in test_sim_feat:\n",
    "    with open(\"../data/{}.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(globals()[feat], handler, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_jaccard(str1, str2):\n",
    "    \"\"\"\n",
    "    Similar to the get_common_ratio, but with different T, this measure considers the relative order among words\n",
    "    \"\"\"\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.jaccard(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "def str_nlevenshtein_1(str1, str2):\n",
    "    \"\"\"\n",
    "    calculate the levenshtein distance\n",
    "    \"\"\"\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.nlevenshtein(str1_list, str2_list,method=1)\n",
    "    return res\n",
    "\n",
    "def str_nlevenshtein_2(str1, str2):\n",
    "    \"\"\"\n",
    "    calculate the levenshtein distance\n",
    "    \"\"\"\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.nlevenshtein(str1_list, str2_list,method=2)\n",
    "    return res\n",
    "\n",
    "def str_sorensen(str1, str2):\n",
    "    \"\"\"\n",
    "    calculate the levenshtein distance\n",
    "    \"\"\"\n",
    "    str1_list = str1.split(\" \")\n",
    "    str2_list = str2.split(\" \")\n",
    "    res = distance.sorensen(str1_list, str2_list)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract distance\n",
    "train_jaccard = train.apply(lambda x: str_jaccard(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_jaccard_stem = train.apply(lambda x: str_jaccard(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "test_jaccard = test.apply(lambda x: str_jaccard(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_jaccard_stem = test.apply(lambda x: str_jaccard(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "\n",
    "train_nlevenshtein_1 = train.apply(lambda x: str_nlevenshtein_1(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_nlevenshtein_1_stem = train.apply(lambda x: str_nlevenshtein_1(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "test_nlevenshtein_1 = test.apply(lambda x: str_nlevenshtein_1(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_nlevenshtein_1_stem = test.apply(lambda x: str_nlevenshtein_1(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "\n",
    "train_nlevenshtein_2 = train.apply(lambda x: str_nlevenshtein_2(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_nlevenshtein_2_stem = train.apply(lambda x: str_nlevenshtein_2(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "test_nlevenshtein_2 = test.apply(lambda x: str_nlevenshtein_2(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_nlevenshtein_2_stem = test.apply(lambda x: str_nlevenshtein_2(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "\n",
    "train_sorensen = train.apply(lambda x: str_sorensen(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_sorensen_stem = train.apply(lambda x: str_sorensen(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "test_sorensen = test.apply(lambda x: str_sorensen(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_sorensen_stem = test.apply(lambda x: str_sorensen(x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dist_feat = [\"train_jaccard\",\"train_jaccard_stem\",\"train_nlevenshtein_1\",\n",
    "                  'train_nlevenshtein_1_stem','train_nlevenshtein_2','train_nlevenshtein_2_stem',\n",
    "                  \"train_sorensen\", \"train_sorensen_stem\"]\n",
    "\n",
    "for feat in train_dist_feat:\n",
    "    with open(\"../data/{}.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(globals()[feat], handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dist_feat = [\"test_jaccard\",\"test_jaccard_stem\",\"test_nlevenshtein_1\",\n",
    "                  'test_nlevenshtein_1_stem','test_nlevenshtein_2','test_nlevenshtein_2_stem',\n",
    "                  \"test_sorensen\", \"test_sorensen_stem\"]\n",
    "\n",
    "for feat in test_dist_feat:\n",
    "    with open(\"../data/{}.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(globals()[feat], handler, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_len(string):\n",
    "    return len(str(string).split())\n",
    "\n",
    "def char_len(string):\n",
    "    return len(str(string).replace(\" \",\"\"))\n",
    "\n",
    "def word_len_diff(text_a, text_b):\n",
    "    return abs(word_len(text_a) - word_len(text_b))\n",
    "\n",
    "def char_len_diff(text_a, text_b):\n",
    "    return abs(char_len(text_a) - char_len(text_b))\n",
    "\n",
    "def word_match_share(text_a,text_b):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(text_a).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(text_b).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract length\n",
    "len_feats = [\"word_len_diff\",\"char_len_diff\",\"word_match_share\"]\n",
    "\n",
    "for feat in len_feats:\n",
    "    train[feat] = train.apply(lambda x: globals()[feat](x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "    train[feat+\"_stem\"] = train.apply(lambda x: globals()[feat](x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "    test[feat] = test.apply(lambda x: globals()[feat](x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "    test[feat+\"_stem\"] = test.apply(lambda x: globals()[feat](x[\"q1_stem\"],x[\"q2_stem\"]),axis = 1)\n",
    "\n",
    "for feat in len_feats:\n",
    "    with open(\"../data/train_{}.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(train[feat], handler, protocol=2)\n",
    "    with open(\"../data/train_{}_stem.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(train[feat+\"_stem\"], handler, protocol=2)\n",
    "    with open(\"../data/test_{}.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(test[feat], handler, protocol=2)\n",
    "    with open(\"../data/test_{}_stem.pickle\".format(feat) ,\"wb\") as handler:\n",
    "        pickle.dump(test[feat+\"_stem\"], handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ft = ['question1','question2','q1_stem','q2_stem']\n",
    "train_input = train[ft]\n",
    "test_input = test[ft]\n",
    "\n",
    "# print('Generate tfidf')\n",
    "# # len_train = train.shape[0]\n",
    "# data_all = pd.concat([train_input[\"question1\"],train_input[\"question2\"],\n",
    "#                      test_input[\"question1\"],test_input[\"question2\"]])\n",
    "\n",
    "# vect_orig = TfidfVectorizer(max_features=None,ngram_range=(1,1), min_df=3)\n",
    "\n",
    "# corpus = data_all.astype(str).values\n",
    "\n",
    "# vect_orig.fit(corpus)\n",
    "\n",
    "# tfidfs = vect_orig.transform(data_all)\n",
    "# train_tfidf = tfidfs[:train_input.shape[0]*2]\n",
    "# test_tfidf = tfidfs[2*train_input.shape[0]:]\n",
    "\n",
    "# train_tfidf_q1 = train_tfidf[:train_input.shape[0]]\n",
    "# train_tfidf_q2 = train_tfidf[train_input.shape[0]:]\n",
    "# test_tfidf_q1 = test_tfidf[:train_input.shape[0]]\n",
    "# test_tfidf_q2 = test_tfidf[train_input.shape[0]:]\n",
    "\n",
    "# ft = ['question1','question2','q1_stem','q2_stem']\n",
    "# train_input = train[ft]\n",
    "# test_input = test[ft]\n",
    "\n",
    "\n",
    "# print('Generate tfidf stemming')\n",
    "# # len_train = train.shape[0]\n",
    "# data_all = pd.concat([train_input['q1_stem'],train_input['q2_stem'],\n",
    "#                      test_input['q1_stem'],test_input['q2_stem']])\n",
    "\n",
    "# vect_orig = TfidfVectorizer(max_features=None,ngram_range=(1,1), min_df=3)\n",
    "\n",
    "# corpus = data_all.astype(str).values\n",
    "\n",
    "# vect_orig.fit(corpus)\n",
    "\n",
    "# tfidfs = vect_orig.transform(data_all)\n",
    "# train_tfidf = tfidfs[:train_input.shape[0]*2]\n",
    "# test_tfidf = tfidfs[2*train_input.shape[0]:]\n",
    "\n",
    "# train_tfidf_q1_stem = train_tfidf[:train_input.shape[0]]\n",
    "# train_tfidf_q2_stem = train_tfidf[train_input.shape[0]:]\n",
    "# test_tfidf_q1_stem = test_tfidf[:train_input.shape[0]]\n",
    "# test_tfidf_q2_stem = test_tfidf[train_input.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_tiidf(q_train,q_test):\n",
    "    \"Make the steps above in the function\"\n",
    "    data_all = pd.concat([q_train.ix[:,0],q_train.ix[:,1],\n",
    "                     q_test.ix[:,0],q_test.ix[:,1]])\n",
    "    \n",
    "    vect_orig = TfidfVectorizer(max_features=None,ngram_range=(1,1), min_df=3)\n",
    "    corpus = data_all.astype(str).values\n",
    "    vect_orig.fit(corpus)\n",
    "\n",
    "    tfidfs = vect_orig.transform(data_all)\n",
    "    train_tfidf = tfidfs[:q_train.shape[0]*2]\n",
    "    test_tfidf = tfidfs[2*q_train.shape[0]:]\n",
    "\n",
    "    train_tfidf_q1 = train_tfidf[:train_input.shape[0]]\n",
    "    train_tfidf_q2 = train_tfidf[train_input.shape[0]:]\n",
    "    test_tfidf_q1 = test_tfidf[:test_input.shape[0]]\n",
    "    test_tfidf_q2 = test_tfidf[test_input.shape[0]:]\n",
    "\n",
    "    return csr_matrix(train_tfidf_q1), csr_matrix(train_tfidf_q2), \\\n",
    "            csr_matrix(test_tfidf_q1), csr_matrix(test_tfidf_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tfidf_q1, train_tfidf_q2, test_tfidf_q1, test_tfidf_q2 = \\\n",
    "    generate_tiidf(train_input[[\"question1\",\"question2\"]],test_input[[\"question1\",\"question2\"]])\n",
    "    \n",
    "train_tfidf_q1_stem, train_tfidf_q2_stem, test_tfidf_q1_stem, test_tfidf_q2_stem = \\\n",
    "    generate_tiidf(train_input[[\"q1_stem\",\"q2_stem\"]],test_input[[\"q1_stem\",\"q2_stem\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/train_tfidf_q1.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_q1, handler, protocol=2)\n",
    "with open(\"../data/train_tfidf_q2.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_q2, handler, protocol=2)\n",
    "with open(\"../data/test_tfidf_q1.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_q1, handler, protocol=2)\n",
    "with open(\"../data/test_tfidf_q2.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_q2, handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/train_tfidf_q1_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_q1_stem, handler, protocol=2)\n",
    "with open(\"../data/train_tfidf_q2_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_q2_stem, handler, protocol=2)\n",
    "with open(\"../data/test_tfidf_q1_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_q1_stem, handler, protocol=2)\n",
    "with open(\"../data/test_tfidf_q2_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_q2_stem, handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/is_duplicate.pickle\", \"wb\") as handler:\n",
    "        pickle.dump(train[\"is_duplicate\"], handler, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reload tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tfidf = [\"train_tfidf_q1\",\"train_tfidf_q2\",\"train_tfidf_q1_stem\",\"train_tfidf_q2_stem\"]\n",
    "for feat in train_tfidf:\n",
    "    with open(\"../data/{}.pickle\".format(feat), \"rb\") as handler:\n",
    "        exec(\"{} = pickle.load(handler)\".format(feat))\n",
    "\n",
    "train_cos_dist = []\n",
    "for i in range(train_tfidf_q1.shape[0]):\n",
    "    dist = cosine(train_tfidf_q1[i,:].toarray(),train_tfidf_q2[i,:].toarray())\n",
    "    train_cos_dist.append(dist)\n",
    "\n",
    "train_cos_dist_stem = []\n",
    "for i in range(train_tfidf_q1_stem.shape[0]):\n",
    "    dist = cosine(train_tfidf_q1_stem[i,:].toarray(),train_tfidf_q2_stem[i,:].toarray())\n",
    "    train_cos_dist_stem.append(dist)\n",
    "\n",
    "with open(\"../data/train_cos_dist.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_cos_dist, handler, protocol=2)\n",
    "\n",
    "with open(\"../data/train_cos_dist_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_cos_dist_stem , handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tfidf = [\"test_tfidf_q1\",\"test_tfidf_q2\",\"test_tfidf_q1_stem\",\"test_tfidf_q2_stem\"]\n",
    "for feat in test_tfidf:\n",
    "    with open(\"../data/{}.pickle\".format(feat), \"rb\") as handler:\n",
    "        exec(\"{} = pickle.load(handler)\".format(feat))\n",
    "\n",
    "test_cos_dist = []\n",
    "for i in range(test_tfidf_q1.shape[0]):\n",
    "    dist = cosine(test_tfidf_q1[i,:].toarray(),test_tfidf_q2[i,:].toarray())\n",
    "    test_cos_dist.append(dist)\n",
    "\n",
    "test_cos_dist_stem = []\n",
    "for i in range(test_tfidf_q1_stem.shape[0]):\n",
    "    dist = cosine(test_tfidf_q1_stem[i,:].toarray(),test_tfidf_q2_stem[i,:].toarray())\n",
    "    test_cos_dist_stem.append(dist)\n",
    "\n",
    "with open(\"../data/test_cos_dist.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_cos_dist, handler, protocol=2)\n",
    "\n",
    "with open(\"../data/test_cos_dist_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_cos_dist_stem , handler, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum diff and mean diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tfidf_sum_dist = []\n",
    "for i in range(train_tfidf_q1.shape[0]):\n",
    "    dist = abs(train_tfidf_q1[i,:].toarray().sum()-train_tfidf_q2[i,:].toarray().sum())\n",
    "    train_tfidf_sum_dist.append(dist)\n",
    "\n",
    "train_tfidf_sum_dist_stem = []\n",
    "for i in range(train_tfidf_q1_stem.shape[0]):\n",
    "    dist = abs(train_tfidf_q1_stem[i,:].toarray().sum()-train_tfidf_q2_stem[i,:].toarray().sum())\n",
    "    train_tfidf_sum_dist_stem.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/train_tfidf_sum_dist.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_sum_dist, handler, protocol=2)\n",
    "with open(\"../data/train_tfidf_sum_dist_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_sum_dist_stem, handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tfidf_sum_dist = []\n",
    "for i in range(test_tfidf_q1.shape[0]):\n",
    "    dist = abs(test_tfidf_q1[i,:].toarray().sum()-test_tfidf_q2[i,:].toarray().sum())\n",
    "    test_tfidf_sum_dist.append(dist)\n",
    "\n",
    "test_tfidf_sum_dist_stem = []\n",
    "for i in range(test_tfidf_q1_stem.shape[0]):\n",
    "    dist = abs(test_tfidf_q1_stem[i,:].toarray().sum()-test_tfidf_q2_stem[i,:].toarray().sum())\n",
    "    test_tfidf_sum_dist_stem.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/test_tfidf_sum_dist.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_sum_dist, handler, protocol=2)\n",
    "with open(\"../data/test_tfidf_sum_dist_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_sum_dist_stem, handler, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tfidf_sum_q1 = []\n",
    "for i in range(train_tfidf_q1.shape[0]):\n",
    "    dist = train_tfidf_q1[i,:].toarray().sum()\n",
    "    train_tfidf_sum_q1.append(dist)\n",
    "\n",
    "train_tfidf_sum_q1_stem = []\n",
    "for i in range(train_tfidf_q1_stem.shape[0]):\n",
    "    dist = train_tfidf_q1_stem[i,:].toarray().sum()\n",
    "    train_tfidf_sum_q1_stem.append(dist)\n",
    "\n",
    "with open(\"../data/train_tfidf_sum_q1.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_sum_q1, handler, protocol=2)\n",
    "\n",
    "with open(\"../data/train_tfidf_sum_q1_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_sum_q1_stem , handler, protocol=2)\n",
    "\n",
    "train_tfidf_sum_q2 = []\n",
    "for i in range(train_tfidf_q2.shape[0]):\n",
    "    dist = train_tfidf_q2[i,:].toarray().sum()\n",
    "    train_tfidf_sum_q2.append(dist)\n",
    "\n",
    "train_tfidf_sum_q2_stem = []\n",
    "for i in range(train_tfidf_q2_stem.shape[0]):\n",
    "    dist = train_tfidf_q2_stem[i,:].toarray().sum()\n",
    "    train_tfidf_sum_q2_stem.append(dist)\n",
    "\n",
    "with open(\"../data/train_tfidf_sum_q2.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_sum_q2, handler, protocol=2)\n",
    "\n",
    "with open(\"../data/train_tfidf_sum_q2_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(train_tfidf_sum_q2_stem , handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tfidf_sum_q1 = []\n",
    "for i in range(test_tfidf_q1.shape[0]):\n",
    "    dist = test_tfidf_q1[i,:].toarray().sum()\n",
    "    test_tfidf_sum_q1.append(dist)\n",
    "\n",
    "test_tfidf_sum_q1_stem = []\n",
    "for i in range(test_tfidf_q1_stem.shape[0]):\n",
    "    dist = test_tfidf_q1_stem[i,:].toarray().sum()\n",
    "    test_tfidf_sum_q1_stem.append(dist)\n",
    "\n",
    "with open(\"../data/test_tfidf_sum_q1.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_sum_q1, handler, protocol=2)\n",
    "\n",
    "with open(\"../data/test_tfidf_sum_q1_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_sum_q1_stem , handler, protocol=2)\n",
    "\n",
    "test_tfidf_sum_q2 = []\n",
    "for i in range(test_tfidf_q2.shape[0]):\n",
    "    dist = test_tfidf_q2[i,:].toarray().sum()\n",
    "    test_tfidf_sum_q2.append(dist)\n",
    "\n",
    "test_tfidf_sum_q2_stem = []\n",
    "for i in range(test_tfidf_q2_stem.shape[0]):\n",
    "    dist = test_tfidf_q2_stem[i,:].toarray().sum()\n",
    "    test_tfidf_sum_q2_stem.append(dist)\n",
    "\n",
    "with open(\"../data/test_tfidf_sum_q2.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_sum_q2, handler, protocol=2)\n",
    "\n",
    "with open(\"../data/test_tfidf_sum_q2_stem.pickle\" ,\"wb\") as handler:\n",
    "        pickle.dump(test_tfidf_sum_q2_stem , handler, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
