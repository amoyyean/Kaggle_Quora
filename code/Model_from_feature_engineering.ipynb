{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "This notebook will train different models on the new features generated in previous feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import difflib\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "seed = 1024\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/is_duplicate.pickle\", \"rb\") as handler:\n",
    "    is_duplicate = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sim_feats = [\"train_common_ratio\",\"train_common_ratio_stem\",\"train_interaction_ratio\",\n",
    "                  'train_interaction_ratio_stem','train_common_bigrams_ratio','train_common_bigrams_ratio_stem',\n",
    "                 \"train_jaccard\",\"train_jaccard_stem\",\"train_nlevenshtein_1\",\n",
    "                  'train_nlevenshtein_1_stem','train_nlevenshtein_2','train_nlevenshtein_2_stem',\n",
    "                  \"train_sorensen\", \"train_sorensen_stem\",\n",
    "                 \"train_word_len_diff\",\"train_char_len_diff\",\"train_word_match_share\",\n",
    "                 \"train_word_len_diff_stem\",\"train_char_len_diff_stem\",\"train_word_match_share_stem\",\n",
    "                 \"train_cos_dist\", \"train_cos_dist_stem\",\"train_tfidf_sum_dist\",\"train_tfidf_sum_dist_stem\",\n",
    "                   \"train_tfidf_sum_q1\",\"train_tfidf_sum_q1_stem\",'train_tfidf_sum_q2',\"train_tfidf_sum_q2_stem\"]\n",
    "train_tfidf = [\"train_tfidf_q1\",\"train_tfidf_q2\",\"train_tfidf_q1_stem\",\"train_tfidf_q2_stem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sim_feats = [\"test_common_ratio\",\"test_common_ratio_stem\",\"test_interaction_ratio\",\n",
    "                  'test_interaction_ratio_stem','test_common_bigrams_ratio','test_common_bigrams_ratio_stem',\n",
    "                \"test_jaccard\",\"test_jaccard_stem\",\"test_nlevenshtein_1\",\n",
    "                  'test_nlevenshtein_1_stem','test_nlevenshtein_2','test_nlevenshtein_2_stem',\n",
    "                  \"test_sorensen\", \"test_sorensen_stem\",\n",
    "                \"test_word_len_diff\",\"test_char_len_diff\",\"test_word_match_share\",\n",
    "                 \"test_word_len_diff_stem\",\"test_char_len_diff_stem\",\"test_word_match_share_stem\",\n",
    "                 \"test_cos_dist\", \"test_cos_dist_stem\",\"test_tfidf_sum_dist\",\"test_tfidf_sum_dist_stem\",\n",
    "                   \"test_tfidf_sum_q1\",\"test_tfidf_sum_q1_stem\",'test_tfidf_sum_q2',\"test_tfidf_sum_q2_stem\"]\n",
    "\n",
    "# test_tfidf = [\"test_tfidf_q1\",\"test_tfidf_q2\",\"test_tfidf_q1_stem\",\"test_tfidf_q2_stem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load train data \n",
    "for feat in train_sim_feats:\n",
    "    with open(\"../data/{}.pickle\".format(feat), \"rb\") as handler:\n",
    "        exec(\"{} = np.array(pickle.load(handler)).reshape(404290,-1)\".format(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load test data \n",
    "for feat in test_sim_feats:\n",
    "    with open(\"../data/{}.pickle\".format(feat), \"rb\") as handler:\n",
    "        exec(\"{} = np.array(pickle.load(handler)).reshape(2345796,-1)\".format(feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstruct train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "for feat in train_sim_feats:\n",
    "    assert len(eval(\"{}.shape\".format(feat))) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feat in test_sim_feats:\n",
    "    assert len(eval(\"{}.shape\".format(feat))) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = hstack([globals()[comp] for comp in train_sim_feats]).tocsr()\n",
    "train = pd.concat([pd.DataFrame(globals()[comp]) for comp in train_sim_feats],axis=1)\n",
    "train.columns = train_sim_feats\n",
    "train[\"is_duplicate\"] = is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = hstack([globals()[comp] for comp in train_sim_feats]).tocsr()\n",
    "test = pd.concat([pd.DataFrame(globals()[comp]) for comp in test_sim_feats],axis=1)\n",
    "test.columns = test_sim_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_common_ratio</th>\n",
       "      <th>train_common_ratio_stem</th>\n",
       "      <th>train_interaction_ratio</th>\n",
       "      <th>train_interaction_ratio_stem</th>\n",
       "      <th>train_common_bigrams_ratio</th>\n",
       "      <th>train_common_bigrams_ratio_stem</th>\n",
       "      <th>train_jaccard</th>\n",
       "      <th>train_jaccard_stem</th>\n",
       "      <th>train_nlevenshtein_1</th>\n",
       "      <th>train_nlevenshtein_1_stem</th>\n",
       "      <th>...</th>\n",
       "      <th>train_word_match_share_stem</th>\n",
       "      <th>train_cos_dist</th>\n",
       "      <th>train_cos_dist_stem</th>\n",
       "      <th>train_tfidf_sum_dist</th>\n",
       "      <th>train_tfidf_sum_dist_stem</th>\n",
       "      <th>train_tfidf_sum_q1</th>\n",
       "      <th>train_tfidf_sum_q1_stem</th>\n",
       "      <th>train_tfidf_sum_q2</th>\n",
       "      <th>train_tfidf_sum_q2_stem</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192891</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.296471</td>\n",
       "      <td>0.348425</td>\n",
       "      <td>0.243042</td>\n",
       "      <td>0.251994</td>\n",
       "      <td>1.963930</td>\n",
       "      <td>1.962776</td>\n",
       "      <td>1.720888</td>\n",
       "      <td>1.710782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192892</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.709881</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.207441</td>\n",
       "      <td>0.207526</td>\n",
       "      <td>2.377108</td>\n",
       "      <td>2.369066</td>\n",
       "      <td>2.169667</td>\n",
       "      <td>2.161540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        train_common_ratio  train_common_ratio_stem  train_interaction_ratio  \\\n",
       "192891            0.571429                 0.571429                 0.400000   \n",
       "192892            0.363636                 0.727273                 0.222222   \n",
       "\n",
       "        train_interaction_ratio_stem  train_common_bigrams_ratio  \\\n",
       "192891                      0.400000                         0.4   \n",
       "192892                      0.571429                         0.0   \n",
       "\n",
       "        train_common_bigrams_ratio_stem  train_jaccard  train_jaccard_stem  \\\n",
       "192891                         0.400000            0.5               0.500   \n",
       "192892                         0.444444            0.7               0.375   \n",
       "\n",
       "        train_nlevenshtein_1  train_nlevenshtein_1_stem      ...       \\\n",
       "192891              0.400000                   0.400000      ...        \n",
       "192892              0.571429                   0.285714      ...        \n",
       "\n",
       "        train_word_match_share_stem  train_cos_dist  train_cos_dist_stem  \\\n",
       "192891                     0.571429        0.296471             0.348425   \n",
       "192892                     0.727273        0.709881             0.312223   \n",
       "\n",
       "        train_tfidf_sum_dist  train_tfidf_sum_dist_stem  train_tfidf_sum_q1  \\\n",
       "192891              0.243042                   0.251994            1.963930   \n",
       "192892              0.207441                   0.207526            2.377108   \n",
       "\n",
       "        train_tfidf_sum_q1_stem  train_tfidf_sum_q2  train_tfidf_sum_q2_stem  \\\n",
       "192891                 1.962776            1.720888                 1.710782   \n",
       "192892                 2.369066            2.169667                 2.161540   \n",
       "\n",
       "        is_duplicate  \n",
       "192891             0  \n",
       "192892             0  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_common_ratio</th>\n",
       "      <th>test_common_ratio_stem</th>\n",
       "      <th>test_interaction_ratio</th>\n",
       "      <th>test_interaction_ratio_stem</th>\n",
       "      <th>test_common_bigrams_ratio</th>\n",
       "      <th>test_common_bigrams_ratio_stem</th>\n",
       "      <th>test_jaccard</th>\n",
       "      <th>test_jaccard_stem</th>\n",
       "      <th>test_nlevenshtein_1</th>\n",
       "      <th>test_nlevenshtein_1_stem</th>\n",
       "      <th>...</th>\n",
       "      <th>test_char_len_diff_stem</th>\n",
       "      <th>test_word_match_share_stem</th>\n",
       "      <th>test_cos_dist</th>\n",
       "      <th>test_cos_dist_stem</th>\n",
       "      <th>test_tfidf_sum_dist</th>\n",
       "      <th>test_tfidf_sum_dist_stem</th>\n",
       "      <th>test_tfidf_sum_q1</th>\n",
       "      <th>test_tfidf_sum_q1_stem</th>\n",
       "      <th>test_tfidf_sum_q2</th>\n",
       "      <th>test_tfidf_sum_q2_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2345794</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.280878</td>\n",
       "      <td>0.309392</td>\n",
       "      <td>0.213820</td>\n",
       "      <td>0.226343</td>\n",
       "      <td>3.102861</td>\n",
       "      <td>3.077048</td>\n",
       "      <td>3.316681</td>\n",
       "      <td>3.303391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345795</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.358283</td>\n",
       "      <td>0.309387</td>\n",
       "      <td>0.240722</td>\n",
       "      <td>0.236988</td>\n",
       "      <td>2.140926</td>\n",
       "      <td>2.134253</td>\n",
       "      <td>1.900204</td>\n",
       "      <td>1.897265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_common_ratio  test_common_ratio_stem  test_interaction_ratio  \\\n",
       "2345794           0.869565                0.869565                0.769231   \n",
       "2345795           0.444444                0.444444                0.285714   \n",
       "\n",
       "         test_interaction_ratio_stem  test_common_bigrams_ratio  \\\n",
       "2345794                     0.769231                   0.666667   \n",
       "2345795                     0.285714                   0.285714   \n",
       "\n",
       "         test_common_bigrams_ratio_stem  test_jaccard  test_jaccard_stem  \\\n",
       "2345794                        0.666667      0.214286           0.214286   \n",
       "2345795                        0.285714      0.625000           0.625000   \n",
       "\n",
       "         test_nlevenshtein_1  test_nlevenshtein_1_stem  \\\n",
       "2345794                0.125                     0.125   \n",
       "2345795                0.500                     0.500   \n",
       "\n",
       "                  ...            test_char_len_diff_stem  \\\n",
       "2345794           ...                                 10   \n",
       "2345795           ...                                  8   \n",
       "\n",
       "         test_word_match_share_stem  test_cos_dist  test_cos_dist_stem  \\\n",
       "2345794                    0.869565       0.280878            0.309392   \n",
       "2345795                    0.444444       0.358283            0.309387   \n",
       "\n",
       "         test_tfidf_sum_dist  test_tfidf_sum_dist_stem  test_tfidf_sum_q1  \\\n",
       "2345794             0.213820                  0.226343           3.102861   \n",
       "2345795             0.240722                  0.236988           2.140926   \n",
       "\n",
       "         test_tfidf_sum_q1_stem  test_tfidf_sum_q2  test_tfidf_sum_q2_stem  \n",
       "2345794                3.077048           3.316681                3.303391  \n",
       "2345795                2.134253           1.900204                1.897265  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.columns = train_sim_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resampling\n",
    "pos_train = train[train['is_duplicate'] == 1]\n",
    "neg_train = train[train['is_duplicate'] == 0]\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "    \n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "train = pd.concat([pos_train, neg_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict with train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[train_sim_feats], train[\"is_duplicate\"],\n",
    "                                                    test_size = 0.3, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'min_child_weight': 1,\n",
    "    'eta': 0.01,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'max_depth': 12,\n",
    "    'subsample': 0.3,\n",
    "    'reg_alpha': 1,\n",
    "    'gamma': 0.04,\n",
    "    'silent':True,\n",
    "    \"eval_metric\":\"logloss\"}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "xgval = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.687978\tvalidation-logloss:0.688037\n",
      "Multiple eval metrics have been passed: 'validation-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.516952\tvalidation-logloss:0.519952\n",
      "[100]\ttrain-logloss:0.435586\tvalidation-logloss:0.441448\n",
      "[150]\ttrain-logloss:0.391846\tvalidation-logloss:0.400636\n",
      "[200]\ttrain-logloss:0.365837\tvalidation-logloss:0.377812\n",
      "[250]\ttrain-logloss:0.349085\tvalidation-logloss:0.364409\n",
      "[300]\ttrain-logloss:0.337601\tvalidation-logloss:0.356148\n",
      "[350]\ttrain-logloss:0.329262\tvalidation-logloss:0.350797\n",
      "[400]\ttrain-logloss:0.323048\tvalidation-logloss:0.347175\n",
      "[450]\ttrain-logloss:0.317757\tvalidation-logloss:0.344426\n",
      "[500]\ttrain-logloss:0.313364\tvalidation-logloss:0.34253\n",
      "[550]\ttrain-logloss:0.30956\tvalidation-logloss:0.340998\n",
      "[600]\ttrain-logloss:0.305895\tvalidation-logloss:0.339515\n",
      "[650]\ttrain-logloss:0.302324\tvalidation-logloss:0.337993\n",
      "[700]\ttrain-logloss:0.299138\tvalidation-logloss:0.336889\n",
      "[750]\ttrain-logloss:0.296512\tvalidation-logloss:0.336053\n",
      "[800]\ttrain-logloss:0.293671\tvalidation-logloss:0.334909\n",
      "[850]\ttrain-logloss:0.290819\tvalidation-logloss:0.333985\n",
      "[900]\ttrain-logloss:0.288167\tvalidation-logloss:0.333762\n",
      "[950]\ttrain-logloss:0.285747\tvalidation-logloss:0.332699\n",
      "[1000]\ttrain-logloss:0.283181\tvalidation-logloss:0.33191\n",
      "[1050]\ttrain-logloss:0.280792\tvalidation-logloss:0.331178\n",
      "[1100]\ttrain-logloss:0.278517\tvalidation-logloss:0.330384\n",
      "[1150]\ttrain-logloss:0.276174\tvalidation-logloss:0.329599\n",
      "[1200]\ttrain-logloss:0.273675\tvalidation-logloss:0.328989\n",
      "[1250]\ttrain-logloss:0.271556\tvalidation-logloss:0.328125\n",
      "[1300]\ttrain-logloss:0.269272\tvalidation-logloss:0.327339\n",
      "[1350]\ttrain-logloss:0.267226\tvalidation-logloss:0.326394\n",
      "[1400]\ttrain-logloss:0.265168\tvalidation-logloss:0.325678\n",
      "[1450]\ttrain-logloss:0.263111\tvalidation-logloss:0.325017\n",
      "[1500]\ttrain-logloss:0.26114\tvalidation-logloss:0.324198\n",
      "[1550]\ttrain-logloss:0.259208\tvalidation-logloss:0.32382\n",
      "[1600]\ttrain-logloss:0.257272\tvalidation-logloss:0.323026\n",
      "[1650]\ttrain-logloss:0.255213\tvalidation-logloss:0.321978\n",
      "[1700]\ttrain-logloss:0.253256\tvalidation-logloss:0.321364\n",
      "[1750]\ttrain-logloss:0.251488\tvalidation-logloss:0.320795\n",
      "[1800]\ttrain-logloss:0.249636\tvalidation-logloss:0.320282\n",
      "[1850]\ttrain-logloss:0.247766\tvalidation-logloss:0.319697\n",
      "[1900]\ttrain-logloss:0.245921\tvalidation-logloss:0.319335\n",
      "[1950]\ttrain-logloss:0.244126\tvalidation-logloss:0.318498\n"
     ]
    }
   ],
   "source": [
    "# fit model no training data\n",
    "gb_model = xgb.train(params, \n",
    "                     dtrain=xgtrain, \n",
    "                     verbose_eval = 50,\n",
    "                     evals=[(xgtrain,\"train\"),(xgval,\"validation\")], \n",
    "                     early_stopping_rounds = 50,\n",
    "                     num_boost_round = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with how train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgtest = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = gb_model.predict(xgtest,ntree_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = np.array(range(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub =  pd.DataFrame({\"test_id\":test_id,\"is_duplicate\":pred},columns=[\"test_id\",\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"../submit/sub11.csv\",index=False, header=True, cols=[\"test_id\",\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
