{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook focus on the application of LSTM to embed the sentence and measure the similarity between question pairs\n",
    "<br>\n",
    "Reference: ***Siamese Recurrent Architectures for Learning Sentence Similarity***\n",
    "<br>http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import re\n",
    "import _pickle as pickle \n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow import nn\n",
    "from sys import getsizeof\n",
    "import time\n",
    "from tflearn.data_utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = pickle.load(open(\"w2v_model.dat\", \"rb\"))\n",
    "\n",
    "w2v_table = {}\n",
    "\n",
    "for i in w2v_model.wv.vocab.keys():\n",
    "    w2v_table[i] = w2v_model[i].tolist()\n",
    "\n",
    "w2v_table[\"1992.0\"] = [0]*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(w2v_table, open(\"w2v_table.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_q1 = np.array(pickle.load(open(\"../data/train_q1.dat\", \"rb\"))[0:10000])\n",
    "train_q2 = np.array(pickle.load(open(\"../data/train_q2.dat\", \"rb\"))[0:10000])\n",
    "\n",
    "# test_q1 = np.array(pickle.load(open(\"../data/train_q1.dat\", \"rb\"))[0:50000])\n",
    "# test_q2 = np.array(pickle.load(open(\"../data/train_q2.dat\", \"rb\"))[0:50000])\n",
    "\n",
    "target = pickle.load(open(\"../data/y.dat\", \"rb\"))[0:10000]\n",
    "target = np.array(target).reshape(-1,1)\n",
    "target_inverse = 1-target\n",
    "target = np.concatenate([target, target_inverse], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_hidden = 64\n",
    "n_input = 300\n",
    "n_classes = 2\n",
    "n_dense1 = 256\n",
    "# n_dense2 = 256\n",
    "\n",
    "learning_rate = 0.01\n",
    "lambda_loss_amount = 0.001\n",
    "batch = 512\n",
    "training_iters = 20*batch\n",
    "display_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset all parameters\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph input\n",
    "x1 = tf.placeholder(tf.float64, [None, None, n_input])\n",
    "x2 = tf.placeholder(tf.float64, [None, None, n_input])\n",
    "y_ = tf.placeholder(tf.float64, [None, n_classes])\n",
    "sentence_1_length = tf.placeholder(tf.int32, [None])\n",
    "sentence_2_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# dense layer variable\n",
    "W = tf.Variable(tf.random_normal([2*n_hidden,n_classes],dtype=tf.float64), name=\"W1\")\n",
    "b = tf.Variable(tf.random_normal([n_classes],dtype=tf.float64), name=\"b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_relevant(output, length):\n",
    "    batch_size = tf.shape(output)[0]\n",
    "    max_length = tf.shape(output)[1]\n",
    "    out_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN embedding\n",
    "with tf.variable_scope('embedding_1'):\n",
    "    rnn_cell_1 = rnn.GRUCell(n_hidden)\n",
    "    outputs_1, _ = nn.dynamic_rnn(rnn_cell_1, x1, sequence_length = sentence_1_length, dtype=tf.float64)\n",
    "\n",
    "with tf.variable_scope('embedding_2'):\n",
    "    rnn_cell_2 = rnn.GRUCell(n_hidden)\n",
    "    outputs_2, _ = nn.dynamic_rnn(rnn_cell_2, x2, sequence_length = sentence_2_length, dtype=tf.float64)\n",
    "\n",
    "# get similarity measure\n",
    "last1 = last_relevant(outputs_1, sentence_1_length)\n",
    "last2 = last_relevant(outputs_2, sentence_2_length)\n",
    "\n",
    "# diff = tf.subtract(last1, last2)\n",
    "# l1_distance = tf.abs(diff)\n",
    "\n",
    "# summed_l1_distance = tf.reduce_sum(l1_distance, axis=1,\n",
    "#                                    keep_dims=True)\n",
    "\n",
    "# positive_class_probs = tf.exp(-summed_l1_distance)\n",
    "# negative_class_probs = 1 - positive_class_probs\n",
    "\n",
    "sentence_embedding = tf.concat([last1, last2], axis=1)\n",
    "y = tf.matmul(sentence_embedding,W) + b\n",
    "# y = tf.concat([negative_class_probs, positive_class_probs], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiran/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# SGD\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "batch_test_1 = train_q1[0:1000]\n",
    "batch_test_len_1 = [len(x) for x in batch_test_1]\n",
    "batch_test_pad_1 = pad_sequences(batch_test_1, value = 1992, dtype = \"object\").astype(\"str\")\n",
    "batch_test_trans_1 = [[w2v_table[w] for w in s] for s in batch_test_pad_1]\n",
    "\n",
    "batch_test_2 = train_q2[0:1000]\n",
    "batch_test_len_2 = [len(x) for x in batch_test_2]\n",
    "batch_test_pad_2 = pad_sequences(batch_test_2, value = 1992, dtype = \"object\").astype(\"str\")\n",
    "batch_test_trans_2 = [[w2v_table[w] for w in s] for s in batch_test_pad_2]\n",
    "\n",
    "batch_test_y = target[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.80719291013\n",
      "Train Accuracy: 0.594\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.768096237247\n",
      "Train Accuracy: 0.607\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.738199390254\n",
      "Train Accuracy: 0.621\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.714342500671\n",
      "Train Accuracy: 0.634\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.702135510837\n",
      "Train Accuracy: 0.632\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.683664856809\n",
      "Train Accuracy: 0.643\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.674908399836\n",
      "Train Accuracy: 0.647\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.662560507469\n",
      "Train Accuracy: 0.648\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.6564954168\n",
      "Train Accuracy: 0.648\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.647746679053\n",
      "Train Accuracy: 0.652\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.639714281529\n",
      "Train Accuracy: 0.657\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.633548023341\n",
      "Train Accuracy: 0.66\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.627829836546\n",
      "Train Accuracy: 0.663\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.622610005511\n",
      "Train Accuracy: 0.666\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.621950545057\n",
      "Train Accuracy: 0.666\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.613265990817\n",
      "Train Accuracy: 0.673\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.61331178249\n",
      "Train Accuracy: 0.677\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.605423739316\n",
      "Train Accuracy: 0.673\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.601925307242\n",
      "Train Accuracy: 0.677\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.599047352172\n",
      "Train Accuracy: 0.68\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.596738453807\n",
      "Train Accuracy: 0.682\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.596707033687\n",
      "Train Accuracy: 0.676\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.593579080436\n",
      "Train Accuracy: 0.681\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.587150675635\n",
      "Train Accuracy: 0.689\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.584601276009\n",
      "Train Accuracy: 0.684\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.584945672027\n",
      "Train Accuracy: 0.686\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.584318205172\n",
      "Train Accuracy: 0.687\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.581162301079\n",
      "Train Accuracy: 0.687\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.576167970996\n",
      "Train Accuracy: 0.687\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.576740972945\n",
      "Train Accuracy: 0.691\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.57443855722\n",
      "Train Accuracy: 0.69\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.571001538386\n",
      "Train Accuracy: 0.69\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.570523934678\n",
      "Train Accuracy: 0.692\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.567845001226\n",
      "Train Accuracy: 0.688\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.568403172539\n",
      "Train Accuracy: 0.697\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.565100385725\n",
      "Train Accuracy: 0.694\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.567949959644\n",
      "Train Accuracy: 0.697\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.562264222471\n",
      "Train Accuracy: 0.695\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.564280452325\n",
      "Train Accuracy: 0.698\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.560324200993\n",
      "Train Accuracy: 0.693\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.55852594392\n",
      "Train Accuracy: 0.7\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.559174609999\n",
      "Train Accuracy: 0.697\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.558757969441\n",
      "Train Accuracy: 0.701\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.555144992416\n",
      "Train Accuracy: 0.702\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.556062762127\n",
      "Train Accuracy: 0.701\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.55329979857\n",
      "Train Accuracy: 0.702\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.553382106004\n",
      "Train Accuracy: 0.706\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.551415251513\n",
      "Train Accuracy: 0.703\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.552447638421\n",
      "Train Accuracy: 0.704\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.551712827721\n",
      "Train Accuracy: 0.704\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.549325880699\n",
      "Train Accuracy: 0.706\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.548458477926\n",
      "Train Accuracy: 0.704\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.546624104628\n",
      "Train Accuracy: 0.711\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.546343928176\n",
      "Train Accuracy: 0.706\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.545364199395\n",
      "Train Accuracy: 0.712\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.545401151758\n",
      "Train Accuracy: 0.707\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.544364403699\n",
      "Train Accuracy: 0.707\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.544392549002\n",
      "Train Accuracy: 0.712\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.543086416509\n",
      "Train Accuracy: 0.711\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.541362734676\n",
      "Train Accuracy: 0.717\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.545354486854\n",
      "Train Accuracy: 0.701\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.540573935521\n",
      "Train Accuracy: 0.712\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.539408877666\n",
      "Train Accuracy: 0.72\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.54003715952\n",
      "Train Accuracy: 0.717\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.538872437891\n",
      "Train Accuracy: 0.713\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.537445435664\n",
      "Train Accuracy: 0.718\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.536687816622\n",
      "Train Accuracy: 0.72\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.538908710301\n",
      "Train Accuracy: 0.72\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.535559720025\n",
      "Train Accuracy: 0.719\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.540722347061\n",
      "Train Accuracy: 0.703\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.534378139818\n",
      "Train Accuracy: 0.723\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.535690073345\n",
      "Train Accuracy: 0.723\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.533244931139\n",
      "Train Accuracy: 0.72\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.544835300011\n",
      "Train Accuracy: 0.69\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.538002290142\n",
      "Train Accuracy: 0.704\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.531572443712\n",
      "Train Accuracy: 0.723\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.530819553577\n",
      "Train Accuracy: 0.725\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.533332248716\n",
      "Train Accuracy: 0.728\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.532551464088\n",
      "Train Accuracy: 0.709\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.532966564174\n",
      "Train Accuracy: 0.71\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.528608753131\n",
      "Train Accuracy: 0.726\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.528815518934\n",
      "Train Accuracy: 0.724\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.529394563329\n",
      "Train Accuracy: 0.717\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.527146943805\n",
      "Train Accuracy: 0.727\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.527601617685\n",
      "Train Accuracy: 0.724\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.52848951814\n",
      "Train Accuracy: 0.729\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.525648700891\n",
      "Train Accuracy: 0.728\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.525632551487\n",
      "Train Accuracy: 0.728\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.524585789577\n",
      "Train Accuracy: 0.725\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.526306070484\n",
      "Train Accuracy: 0.727\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.523771702165\n",
      "Train Accuracy: 0.727\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.522981938129\n",
      "Train Accuracy: 0.73\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.52406498425\n",
      "Train Accuracy: 0.72\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.524448411002\n",
      "Train Accuracy: 0.726\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.522513343124\n",
      "Train Accuracy: 0.727\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.521624228719\n",
      "Train Accuracy: 0.728\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.520648145391\n",
      "Train Accuracy: 0.734\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.520421219162\n",
      "Train Accuracy: 0.731\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.521317316183\n",
      "Train Accuracy: 0.729\n",
      "------------------------------------\n",
      "Train Cross-Entropy Loss 0.520456134071\n",
      "Train Accuracy: 0.731\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    step = 1\n",
    "    # batch generating\n",
    "    N = target.shape[0]\n",
    "    batch_index = np.arange(0,N)\n",
    "    batch_start = np.append(np.arange(0,N,batch),N)\n",
    "    print(\"------------------------------------\")\n",
    "    while step <= 100:\n",
    "        \n",
    "        # shuffle data\n",
    "        np.random.shuffle(batch_index)\n",
    "        \n",
    "        for i in range(len(batch_start)-1):\n",
    "            # Batch processing\n",
    "            batch_x_1 = train_q1[batch_index[batch_start[i]:batch_start[i+1]]].tolist()\n",
    "            batch_x_len_1 = [len(x) for x in batch_x_1]\n",
    "            batch_x_pad_1 = pad_sequences(batch_x_1, value = 1992, dtype = \"object\").astype(\"str\")\n",
    "            batch_x_trans_1 = [[w2v_table[w] for w in s] for s in batch_x_pad_1]\n",
    "            \n",
    "            batch_x_2 = train_q2[batch_index[batch_start[i]:batch_start[i+1]]].tolist()\n",
    "            batch_x_len_2 = [len(x) for x in batch_x_2]\n",
    "            batch_x_pad_2 = pad_sequences(batch_x_2, value = 1992, dtype = \"object\").astype(\"str\")\n",
    "            batch_x_trans_2 = [[w2v_table[w] for w in s] for s in batch_x_pad_2]\n",
    "            \n",
    "            batch_y = target[batch_index[batch_start[i]:batch_start[i+1]],:]\n",
    "            \n",
    "            # train a hidden layer\n",
    "            sess.run(train_step, feed_dict={x1:batch_x_trans_1,\n",
    "                                            x2:batch_x_trans_2,\n",
    "                                            sentence_1_length:batch_x_len_1,\n",
    "                                            sentence_2_length:batch_x_len_2,\n",
    "                                            y_: batch_y})\n",
    "        # print loss and accuracy\n",
    "        if step % 1 == 0:\n",
    "            print(\"Train Cross-Entropy Loss\", sess.run(cross_entropy, feed_dict={x1:batch_test_trans_1,\n",
    "                                                x2:batch_test_trans_2,\n",
    "                                                sentence_1_length:batch_test_len_1,\n",
    "                                                sentence_2_length:batch_test_len_2,\n",
    "                                                y_: batch_test_y}))        \n",
    "            print(\"Train Accuracy:\", sess.run(accuracy, feed_dict={x1:batch_test_trans_1,\n",
    "                                                x2:batch_test_trans_2,\n",
    "                                                sentence_1_length:batch_test_len_1,\n",
    "                                                sentence_2_length:batch_test_len_1,\n",
    "                                                y_: batch_test_y})) \n",
    "#             print(sess.run(last1,feed_dict={x1:batch_test_trans_1,\n",
    "#                                                 x2:batch_test_trans_2,\n",
    "#                                                 sentence_1_length:batch_test_len_1,\n",
    "#                                                 sentence_2_length:batch_test_len_1,\n",
    "#                                                 y_: batch_test_y})) \n",
    "            print(\"------------------------------------\")\n",
    "        # update batch\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
