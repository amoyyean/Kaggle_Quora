{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN embedding\n",
    "This notebook focus on the application of LSTM to embed the sentence and measure the similarity between question pairs\n",
    "<br>\n",
    "Reference: ***Siamese Recurrent Architectures for Learning Sentence Similarity***\n",
    "<br>http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow import nn\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = pickle.load(open(\"w2v_model.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # clean the sentence\n",
    "# def txt_clean(sentence):\n",
    "#     s = re.sub(\"[^a-zA-Z0-9]\", \" \", str(sentence))\n",
    "#     s_list = s.lower().split()\n",
    "#     return s_list\n",
    "\n",
    "# train = pd.read_csv(\"../data/train.csv\")\n",
    "# test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# # transform to get split sentence\n",
    "# train_q1 = train.apply(lambda x: txt_clean(x[\"question1\"]),axis = 1).tolist()\n",
    "# train_q2 = train.apply(lambda x: txt_clean(x[\"question2\"]),axis = 1).tolist()\n",
    "# test_q1 = test.apply(lambda x: txt_clean(x[\"question1\"]),axis = 1).tolist()\n",
    "# test_q2 = test.apply(lambda x: txt_clean(x[\"question2\"]),axis = 1).tolist()\n",
    "# y = train.is_duplicate.tolist()\n",
    "\n",
    "# pickle.dump(train_q1, open(\"../data/train_q1.dat\", \"wb\"))\n",
    "# pickle.dump(train_q2, open(\"../data/train_q2.dat\", \"wb\"))\n",
    "# pickle.dump(test_q1, open(\"../data/test_q1.dat\", \"wb\"))\n",
    "# pickle.dump(test_q2, open(\"../data/test_q2.dat\", \"wb\"))\n",
    "# pickle.dump(y, open(\"../data/y.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_q1 = pickle.load(open(\"../data/train_q1.dat\", \"rb\"))[0:5000]\n",
    "train_q2 = pickle.load(open(\"../data/train_q2.dat\", \"rb\"))[0:5000]\n",
    "# test_q1 = pickle.load(open(\"../data/test_q1.dat\", \"rb\"))[0:1000]\n",
    "# test_q1 = pickle.load(open(\"../data/test_q1.dat\", \"rb\"))[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = pickle.load(open(\"../data/y.dat\", \"rb\"))[0:5000]\n",
    "\n",
    "target = np.array(target).reshape(-1,1)\n",
    "\n",
    "target_inverse = 1-target\n",
    "\n",
    "target = np.concatenate([target, target_inverse], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a LSTM for embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.Test the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28979221 -0.18480104 -0.32667643 -0.17523807 -0.11447719]\n",
      "[ 0.2155818   0.08308727 -0.12565269  0.11223465 -0.3529726 ]\n",
      "[ 0.27783838 -0.04633152 -0.26982322  0.00828765 -0.09711678]\n",
      "[ 0.10206932 -0.0277025  -0.05544248 -0.09461203 -0.0744462 ]\n",
      "[ 0.2376662   0.01902849 -0.14263637 -0.19002569 -0.12894249]\n",
      "[ 0.0527588   0.00629033 -0.09461353 -0.06562302 -0.04679567]\n",
      "[ 0.05161272  0.0124112  -0.08588935 -0.06794626 -0.04058785]\n",
      "[ 0.09765382 -0.15506659 -0.23301676 -0.07342669 -0.04687157]\n",
      "[ 0.03986795  0.01472408 -0.0949952  -0.06355751 -0.04129621]\n",
      "[ 0.04446325  0.00925547 -0.09746711 -0.07145177 -0.04257616]\n"
     ]
    }
   ],
   "source": [
    "# trainsform to word vectors\n",
    "for w in train_q1[1]:\n",
    "    print(w2v_model.wv[w][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model is OK to transform the word to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2.padding the list to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'step',\n",
       " 'by',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'to',\n",
       " 'invest',\n",
       " 'in',\n",
       " 'share',\n",
       " 'market',\n",
       " 'in',\n",
       " 'india']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'increase',\n",
       " 'the',\n",
       " 'speed',\n",
       " 'of',\n",
       " 'my',\n",
       " 'internet',\n",
       " 'connection',\n",
       " 'while',\n",
       " 'using',\n",
       " 'a',\n",
       " 'vpn']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata = train_q1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions for padding list\n",
    "def getMaxLength(sentences):\n",
    "    \"setences: list of list of words\"\n",
    "    maxLen = -1\n",
    "    for s in sentences:\n",
    "        if len(s) > maxLen:\n",
    "            maxLen = len(s)\n",
    "    return maxLen\n",
    "\n",
    "def padding(sentences, maxLen):\n",
    "    padContent = \"thisispadding\"\n",
    "    sentencesPad = []\n",
    "    for s in sentences:\n",
    "        s.extend([padContent]*(maxLen-len(s)))\n",
    "        sentencesPad.append(s)\n",
    "    return sentencesPad\n",
    "\n",
    "def transToVec(sentences,maxLen):\n",
    "    transSentence = []\n",
    "    for s in sentences:\n",
    "        sen = []\n",
    "        for j in range(maxLen):\n",
    "            if s[j] == \"thisispadding\":\n",
    "                sen.append([0] * n_input)\n",
    "            else:\n",
    "                sen.append(w2v_model.wv[s[j]].tolist())\n",
    "        transSentence.append(sen)\n",
    "    return np.array(transSentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for loop to padd\n",
    "maxLen = getMaxLength(testdata)\n",
    "\n",
    "testdatapad = padding(testdata, maxLen)\n",
    "\n",
    "transdata = transToVec(testdatapad,maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 17, 300)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using traditional RNN cell first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.Model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset all parameters\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_hidden = 64\n",
    "n_input = 300\n",
    "n_classes = 2\n",
    "\n",
    "learning_rate = 0.01\n",
    "lambda_loss_amount = 0.001\n",
    "batch_size = 128\n",
    "training_iters = 20*batch_size\n",
    "display_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph input\n",
    "x1 = tf.placeholder(tf.float64, [None, None, n_input])\n",
    "x2 = tf.placeholder(tf.float64, [None, None, n_input])\n",
    "y_ = tf.placeholder(tf.float64, [None, n_classes])\n",
    "\n",
    "# dense layer variable\n",
    "W = tf.Variable(tf.random_normal([2*n_hidden,n_classes],dtype=tf.float64))\n",
    "b = tf.Variable(tf.random_normal([n_classes],dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN embedding\n",
    "def RNN(x1, x2, weights, bias):\n",
    "    \n",
    "    with tf.variable_scope('embedding_1'):\n",
    "        rnn_cell_1 = rnn.BasicRNNCell(n_hidden)\n",
    "        outputs_1, states_1 = nn.dynamic_rnn(rnn_cell_1, x1, dtype=tf.float64)\n",
    "        \n",
    "    with tf.variable_scope('embedding_2'):\n",
    "        rnn_cell_2 = rnn.BasicRNNCell(n_hidden)\n",
    "        outputs_2, states_2 = nn.dynamic_rnn(rnn_cell_2, x2, dtype=tf.float64)\n",
    "    \n",
    "    # concat two embedding into one\n",
    "    sentence_embedding = tf.concat([states_1, states_2], axis=1)\n",
    "        \n",
    "    return tf.matmul(sentence_embedding,weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "y = RNN(x1, x2, W, b)\n",
    "\n",
    "# loss\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# SGD\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch generating\n",
    "N = target.shape[0]\n",
    "batch_index = np.arange(0,N)\n",
    "batch_start = np.append(np.arange(0,N,batch_size),N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model test with 5000 training queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.75\n",
      "Train Accuracy: 0.75\n",
      "Train Accuracy: 0.875\n",
      "Train Accuracy: 1.0\n",
      "Train Accuracy: 1.0\n",
      "Train Accuracy: 1.0\n",
      "Train Accuracy: 1.0\n",
      "Train Accuracy: 1.0\n",
      "Train Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    step = 1\n",
    "    while step < 10:\n",
    "        \n",
    "        # shuffle data\n",
    "        np.random.shuffle(batch_index)\n",
    "        \n",
    "        for i in range(len(batch_start)-1):\n",
    "            \n",
    "            # Batch processing \n",
    "            batch_x_1= train_q1[batch_start[i]:batch_start[i+1]]\n",
    "            maxLen_1 = getMaxLength(batch_x_1)\n",
    "            batch_x_pad_1 = padding(batch_x_1, maxLen_1)\n",
    "            batch_x_trans_1 = transToVec(batch_x_pad_1,maxLen_1)\n",
    "            batch_x_2= train_q2[batch_start[i]:batch_start[i+1]]\n",
    "            maxLen_2 = getMaxLength(batch_x_2)\n",
    "            batch_x_pad_2 = padding(batch_x_2, maxLen_2)\n",
    "            batch_x_trans_2 = transToVec(batch_x_pad_2,maxLen_2)\n",
    "            batch_y = target[batch_start[i]:batch_start[i+1],:]\n",
    "\n",
    "            # train a hidden layer\n",
    "            sess.run(train_step, feed_dict={x1:batch_x_trans_1,\n",
    "                                            x2:batch_x_trans_2,\n",
    "                                            y_: batch_y})\n",
    "\n",
    "        # print loss and accuracy\n",
    "        print(\"Train Cross-Entropy Loss\", sess.run(cross_entropy, feed_dict={x1:batch_x_trans_1,\n",
    "                                            x2:batch_x_trans_2,\n",
    "                                            y_: batch_y}))\n",
    "        \n",
    "        print(\"Train Accuracy:\", sess.run(accuracy, feed_dict={x1:batch_x_trans_1,\n",
    "                                                                x2:batch_x_trans_2,\n",
    "                                                                y_: batch_y}))\n",
    "        # update batch\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
