{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN embedding\n",
    "This notebook focus on the application of LSTM to embed the sentence and measure the similarity between question pairs\n",
    "<br>\n",
    "Reference: ***Siamese Recurrent Architectures for Learning Sentence Similarity***\n",
    "<br>http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow import nn\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = pickle.load(open(\"w2v_model.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # clean the sentence\n",
    "# def txt_clean(sentence):\n",
    "#     s = re.sub(\"[^a-zA-Z0-9]\", \" \", str(sentence))\n",
    "#     s_list = s.lower().split()\n",
    "#     return s_list\n",
    "\n",
    "# train = pd.read_csv(\"../data/train.csv\")\n",
    "# test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# # transform to get split sentence\n",
    "# train_q1 = train.apply(lambda x: txt_clean(x[\"question1\"]),axis = 1).tolist()\n",
    "# train_q2 = train.apply(lambda x: txt_clean(x[\"question2\"]),axis = 1).tolist()\n",
    "# test_q1 = test.apply(lambda x: txt_clean(x[\"question1\"]),axis = 1).tolist()\n",
    "# test_q2 = test.apply(lambda x: txt_clean(x[\"question2\"]),axis = 1).tolist()\n",
    "# y = train.is_duplicate.tolist()\n",
    "\n",
    "# pickle.dump(train_q1, open(\"../data/train_q1.dat\", \"wb\"))\n",
    "# pickle.dump(train_q2, open(\"../data/train_q2.dat\", \"wb\"))\n",
    "# pickle.dump(test_q1, open(\"../data/test_q1.dat\", \"wb\"))\n",
    "# pickle.dump(test_q2, open(\"../data/test_q2.dat\", \"wb\"))\n",
    "# pickle.dump(y, open(\"../data/y.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_q1 = pickle.load(open(\"../data/train_q1.dat\", \"rb\"))\n",
    "train_q2 = pickle.load(open(\"../data/train_q2.dat\", \"rb\"))\n",
    "# test_q1 = pickle.load(open(\"../data/test_q1.dat\", \"rb\"))[0:1000]\n",
    "# test_q1 = pickle.load(open(\"../data/test_q1.dat\", \"rb\"))[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = pickle.load(open(\"../data/y.dat\", \"rb\"))\n",
    "\n",
    "target = np.array(target).reshape(-1,1)\n",
    "\n",
    "target_inverse = 1-target\n",
    "\n",
    "target = np.concatenate([target, target_inverse], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a LSTM for embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.Test the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13234842  0.00763958 -0.08464085 -0.08113342  0.24815077]\n",
      "[ 0.10499778  0.06805725 -0.10598193 -0.03042802  0.03230476]\n",
      "[-0.0287225  -0.07091928  0.00331686 -0.04636408 -0.02650626]\n",
      "[-0.51597834 -0.08152942 -0.45761555  0.61214203 -0.25413761]\n",
      "[-0.00364251  0.19204271 -0.17447464 -0.03469274 -0.12318879]\n",
      "[ 0.04679416 -0.11005198 -0.65378165  0.05505853  0.09023309]\n",
      "[-0.22974356  0.58205104 -0.12804945 -0.42072222 -0.47509399]\n",
      "[ 0.13577995 -0.19249639 -0.3507593   0.00056107  0.06072884]\n",
      "[-0.41628373  0.11394925 -0.59150016 -0.15818399  0.12219065]\n",
      "[-0.03838921  0.43435898 -0.22790641 -0.10768311  0.15719041]\n"
     ]
    }
   ],
   "source": [
    "# trainsform to word vectors\n",
    "for w in train_q1[1]:\n",
    "    print(w2v_model.wv[w][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model is OK to transform the word to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2.padding the list to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'step',\n",
       " 'by',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'to',\n",
       " 'invest',\n",
       " 'in',\n",
       " 'share',\n",
       " 'market',\n",
       " 'in',\n",
       " 'india']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'can',\n",
       " 'i',\n",
       " 'increase',\n",
       " 'the',\n",
       " 'speed',\n",
       " 'of',\n",
       " 'my',\n",
       " 'internet',\n",
       " 'connection',\n",
       " 'while',\n",
       " 'using',\n",
       " 'a',\n",
       " 'vpn']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata = train_q1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for loop to padd\n",
    "maxLen = getMaxLength(testdata)\n",
    "\n",
    "testdatapad = padding(testdata, maxLen)\n",
    "\n",
    "transdata = transToVec(testdatapad,maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 17, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using traditional RNN cell first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.Model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions for padding list\n",
    "def getMaxLength(sentences):\n",
    "    \"setences: list of list of words\"\n",
    "    maxLen = -1\n",
    "    for s in sentences:\n",
    "        if len(s) > maxLen:\n",
    "            maxLen = len(s)\n",
    "    return maxLen\n",
    "\n",
    "def padding(sentences, maxLen):\n",
    "    padContent = \"thisispadding\"\n",
    "    sentencesPad = []\n",
    "    for s in sentences:\n",
    "        s.extend([padContent]*(maxLen-len(s)))\n",
    "        sentencesPad.append(s)\n",
    "    return sentencesPad\n",
    "\n",
    "def transToVec(sentences,maxLen):\n",
    "    transSentence = []\n",
    "    for s in sentences:\n",
    "        sen = []\n",
    "        for j in range(maxLen):\n",
    "            if s[j] == \"thisispadding\":\n",
    "                sen.append([0] * n_input)\n",
    "            else:\n",
    "                sen.append(w2v_model.wv[s[j]].tolist())\n",
    "        transSentence.append(sen)\n",
    "    return np.array(transSentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset all parameters\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_hidden = 64\n",
    "n_input = 300\n",
    "n_classes = 2\n",
    "n_dense1 = 256\n",
    "# n_dense2 = 256\n",
    "\n",
    "learning_rate = 0.01\n",
    "lambda_loss_amount = 0.001\n",
    "batch_size = 512\n",
    "training_iters = 20*batch_size\n",
    "display_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph input\n",
    "x1 = tf.placeholder(tf.float64, [None, None, n_input])\n",
    "x2 = tf.placeholder(tf.float64, [None, None, n_input])\n",
    "y_ = tf.placeholder(tf.float64, [None, n_classes])\n",
    "\n",
    "# dense layer variable\n",
    "W1 = tf.Variable(tf.random_normal([2*n_hidden,n_classes],dtype=tf.float64))\n",
    "b1 = tf.Variable(tf.random_normal([n_classes],dtype=tf.float64))\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([n_dense1,n_classes],dtype=tf.float64))\n",
    "# b2 = tf.Variable(tf.random_normal([n_classes],dtype=tf.float64))\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([n_dense2,n_classes],dtype=tf.float64))\n",
    "# b3 = tf.Variable(tf.random_normal([n_classes],dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN embedding\n",
    "def RNN(x1, x2, W1, b1):\n",
    "    \n",
    "    with tf.variable_scope('embedding_1'):\n",
    "        rnn_cell_1 = rnn.BasicLSTMCell(n_hidden)\n",
    "        outputs_1, states_1 = nn.dynamic_rnn(rnn_cell_1, x1, dtype=tf.float64)\n",
    "        \n",
    "    with tf.variable_scope('embedding_2'):\n",
    "        rnn_cell_2 = rnn.BasicLSTMCell(n_hidden)\n",
    "        outputs_2, states_2 = nn.dynamic_rnn(rnn_cell_2, x2, dtype=tf.float64)\n",
    "\n",
    "    # concat two embedding into one\n",
    "    sentence_embedding = tf.concat([states_1[0], states_2[0]], axis=1)\n",
    "    \n",
    "    with tf.variable_scope(\"dense_layer\"):\n",
    "        y = tf.matmul(sentence_embedding,W1) + b1\n",
    "#     a1 = tf.nn.relu(tf.matmul(sentence_embedding,W1) + b1)\n",
    "#     a2 = tf.matmul(a1,W2) + b2\n",
    "#     a3 = tf.matmul(a2,W3) + b3\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "y = RNN(x1, x2, W1, b1)\n",
    "\n",
    "# loss\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# SGD\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch generating\n",
    "N = target.shape[0]\n",
    "batch_index = np.arange(0,N)\n",
    "batch_start = np.append(np.arange(0,N,batch_size),N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model test with 5000 training queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    step = 1\n",
    "    while step < 100:\n",
    "        \n",
    "        # shuffle data\n",
    "        np.random.shuffle(batch_index)\n",
    "        \n",
    "        for i in range(len(batch_start)-1):\n",
    "            # Batch processing \n",
    "            batch_x_1= train_q1[batch_start[i]:batch_start[i+1]]\n",
    "            maxLen_1 = getMaxLength(batch_x_1)\n",
    "            batch_x_pad_1 = padding(batch_x_1, maxLen_1)\n",
    "            batch_x_trans_1 = transToVec(batch_x_pad_1,maxLen_1)\n",
    "            batch_x_2= train_q2[batch_start[i]:batch_start[i+1]]\n",
    "            maxLen_2 = getMaxLength(batch_x_2)\n",
    "            batch_x_pad_2 = padding(batch_x_2, maxLen_2)\n",
    "            batch_x_trans_2 = transToVec(batch_x_pad_2,maxLen_2)\n",
    "            batch_y = target[batch_start[i]:batch_start[i+1],:]\n",
    "\n",
    "            # train a hidden layer\n",
    "            sess.run(train_step, feed_dict={x1:batch_x_trans_1,\n",
    "                                            x2:batch_x_trans_2,\n",
    "                                            y_: batch_y})\n",
    "\n",
    "        # print loss and accuracy\n",
    "        print(\"Train Cross-Entropy Loss\", sess.run(cross_entropy, feed_dict={x1:batch_x_trans_1,\n",
    "                                            x2:batch_x_trans_2,\n",
    "                                            y_: batch_y}))\n",
    "        \n",
    "        print(\"Train Accuracy:\", sess.run(accuracy, feed_dict={x1:batch_x_trans_1,\n",
    "                                                                x2:batch_x_trans_2,\n",
    "                                                                y_: batch_y}))\n",
    "        # update batch\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
