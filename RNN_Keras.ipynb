{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import difflib\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import pickle\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = '../input/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense1 = np.random.randint(200, 250)\n",
    "num_dense2 = np.random.randint(150, 200)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d %d_%.2f_%.2f'%(num_lstm, num_dense1, num_dense2, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "count = 0\n",
    "amline = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "    amline += 1\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = str(text).lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## generate handcrafted features\n",
    "########################################\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "def get_common_ratio(text_a,text_b):\n",
    "    \"\"\"\n",
    "    ratio of overlapping: 2*M/T\n",
    "    \"\"\"\n",
    "    seq1 = text_to_wordlist(text_a).split()\n",
    "    seq2 = text_to_wordlist(text_b).split()\n",
    "    seqMatch = difflib.SequenceMatcher(a=seq1, b=seq2)\n",
    "    return seqMatch.ratio()\n",
    "\n",
    "def get_interaction_ratio(text_a,text_b):\n",
    "    \"\"\"\n",
    "    ratio of interactions\n",
    "    \"\"\"\n",
    "    seq1 = set(text_to_wordlist(text_a).split())\n",
    "    seq2 = set(text_to_wordlist(text_b).split())\n",
    "    try:\n",
    "        seqMatch = float(len(seq1.intersection(seq2)))/len(seq1.union(seq2))\n",
    "    except ZeroDivisionError:\n",
    "        seqMatch = 0.0\n",
    "    return seqMatch\n",
    "\n",
    "def get_common_bigrams_ratio(text_a,text_b):\n",
    "    seq1 = [\" \".join(i) for i in nltk.ngrams(text_to_wordlist(text_a).split(), 2)]\n",
    "    seq2 = [\" \".join(i) for i in nltk.ngrams(text_to_wordlist(text_b).split(), 2)]\n",
    "    seqMatch = difflib.SequenceMatcher(a=seq1, b=seq2)\n",
    "    return seqMatch.ratio()\n",
    "\n",
    "# extract similarity\n",
    "train_common_ratio = train_df.apply(lambda x: get_common_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_interaction_ratio = train_df.apply(lambda x: get_interaction_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "train_common_bigrams_ratio = train_df.apply(lambda x: get_common_bigrams_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "test_common_ratio = test_df.apply(lambda x: get_common_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_interaction_ratio = test_df.apply(lambda x: get_interaction_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_common_bigrams_ratio = test_df.apply(lambda x: get_common_bigrams_ratio(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "def str_jaccard(str1, str2):\n",
    "    \"\"\"\n",
    "    Similar to the get_common_ratio, but with different T, this measure considers the relative order among words\n",
    "    \"\"\"\n",
    "    str1_list = text_to_wordlist(str1).split(\" \")\n",
    "    str2_list = text_to_wordlist(str2).split(\" \")\n",
    "    res = distance.jaccard(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "def str_nlevenshtein_1(str1, str2):\n",
    "    \"\"\"\n",
    "    calculate the levenshtein distance\n",
    "    \"\"\"\n",
    "    str1_list = text_to_wordlist(str1).split(\" \")\n",
    "    str2_list = text_to_wordlist(str2).split(\" \")\n",
    "    res = distance.nlevenshtein(str1_list, str2_list,method=1)\n",
    "    return res\n",
    "\n",
    "def str_nlevenshtein_2(str1, str2):\n",
    "    \"\"\"\n",
    "    calculate the levenshtein distance\n",
    "    \"\"\"\n",
    "    str1_list = text_to_wordlist(str1).split(\" \")\n",
    "    str2_list = text_to_wordlist(str2).split(\" \")\n",
    "    res = distance.nlevenshtein(str1_list, str2_list,method=2)\n",
    "    return res\n",
    "\n",
    "def str_sorensen(str1, str2):\n",
    "    \"\"\"\n",
    "    calculate the levenshtein distance\n",
    "    \"\"\"\n",
    "    str1_list = text_to_wordlist(str1).split(\" \")\n",
    "    str2_list = text_to_wordlist(str2).split(\" \")\n",
    "    res = distance.sorensen(str1_list, str2_list)\n",
    "    return res\n",
    "\n",
    "# extract distance\n",
    "train_jaccard = train_df.apply(lambda x: str_jaccard(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_jaccard = test_df.apply(lambda x: str_jaccard(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "train_nlevenshtein_1 = train_df.apply(lambda x: str_nlevenshtein_1(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_nlevenshtein_1 = test_df.apply(lambda x: str_nlevenshtein_1(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "train_nlevenshtein_2 = train_df.apply(lambda x: str_nlevenshtein_2(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_nlevenshtein_2 = test_df.apply(lambda x: str_nlevenshtein_2(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "train_sorensen = train_df.apply(lambda x: str_sorensen(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_sorensen = test_df.apply(lambda x: str_sorensen(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "def word_len(string):\n",
    "    return len(text_to_wordlist(string).split())\n",
    "\n",
    "def char_len(string):\n",
    "    return len(text_to_wordlist(string).replace(\" \",\"\"))\n",
    "\n",
    "def word_len_diff(text_a, text_b):\n",
    "    return abs(word_len(text_a) - word_len(text_b))\n",
    "\n",
    "def char_len_diff(text_a, text_b):\n",
    "    return abs(char_len(text_a) - char_len(text_b))\n",
    "\n",
    "def word_match_share(text_a,text_b):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "train_word_len_diff = train_df.apply(lambda x: word_len_diff(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_word_len_diff = test_df.apply(lambda x: word_len_diff(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "train_char_len_diff = train_df.apply(lambda x: char_len_diff(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_char_len_diff = test_df.apply(lambda x: char_len_diff(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "\n",
    "train_word_match_share = train_df.apply(lambda x: word_match_share(x[\"question1\"],x[\"question2\"]),axis = 1)\n",
    "test_word_match_share = test_df.apply(lambda x: word_match_share(x[\"question1\"],x[\"question2\"]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdcft_features = pd.concat([train_common_ratio,\n",
    "                                  train_interaction_ratio,\n",
    "                                  train_common_bigrams_ratio,\n",
    "                                  train_jaccard,\n",
    "                                  train_nlevenshtein_1,\n",
    "                                  train_nlevenshtein_2,\n",
    "                                  train_sorensen,\n",
    "                                  train_word_len_diff,\n",
    "                                  train_char_len_diff,\n",
    "                                  train_word_match_share\n",
    "                                 ], axis=1)\n",
    "\n",
    "test_hdcft_features = pd.concat([test_common_ratio,\n",
    "                                  test_interaction_ratio,\n",
    "                                  test_common_bigrams_ratio,\n",
    "                                  test_jaccard,\n",
    "                                  test_nlevenshtein_1,\n",
    "                                  test_nlevenshtein_2,\n",
    "                                  test_sorensen,\n",
    "                                  test_word_len_diff,\n",
    "                                  test_char_len_diff,\n",
    "                                  test_word_match_share\n",
    "                                 ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((hdcft_features, test_hdcft_features)))\n",
    "hdcft_features = ss.transform(hdcft_features)\n",
    "test_hdcft_features = ss.transform(test_hdcft_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiran/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## generate leaky features\n",
    "########################################\n",
    "ques = pd.concat([train_df[['question1', 'question2']], \\\n",
    "        test_df[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "def q1_freq(row):\n",
    "    return(len(q_dict[row['question1']]))\n",
    "    \n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row['question2']]))\n",
    "    \n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "train_df['q1_q2_intersect'] = train_df.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "train_df['q1_freq'] = train_df.apply(q1_freq, axis=1, raw=True)\n",
    "train_df['q2_freq'] = train_df.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "test_df['q1_q2_intersect'] = test_df.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test_df['q1_freq'] = test_df.apply(q1_freq, axis=1, raw=True)\n",
    "test_df['q2_freq'] = test_df.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "leaks = train_df[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = test_df[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "#np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "hdcft_features_train = np.vstack((hdcft_features[idx_train], hdcft_features[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "hdcft_features_val = np.vstack((hdcft_features[idx_val], hdcft_features[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "#lstm_layer2 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "leaks_input = Input(shape=(leaks.shape[1],))\n",
    "leaks_dense = Dense(num_dense1/2, activation=act)(leaks_input)\n",
    "\n",
    "hdcft_input = Input(shape = (hdcft_features.shape[1],))\n",
    "hdcft_dense = Dense(num_dense1/2, activation=act)(hdcft_input)\n",
    "\n",
    "merged = concatenate([x1, y1, leaks_dense, hdcft_input])\n",
    "\n",
    "#def caldist(cat):\n",
    "#    return tf.reduce_sum(tf.square(tf.subtract(cat[0],cat[1])), axis=1, keep_dims=True)\n",
    "#def calangle(cat):\n",
    "#    return tf.reduce_sum(tf.multiply(cat[0],cat[1]), axis=1, keep_dims=True)\n",
    "\n",
    "#distance = Lambda(caldist)([x1,y1])\n",
    "#angle = Lambda(calangle)([x1,y1])\n",
    "\n",
    "#merged = concatenate([x1, y1, distance, angle, leaks_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(num_dense1, activation=act)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = Dense(num_dense2, activation=act)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, leaks_input, hdcft_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adadelta',\n",
    "              #nadam\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=20)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train, hdcft_features_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, hdcft_features_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks, test_hdcft_features], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks, test_hdcft_features], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lstm_300_206 190_0.35_0.36'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19184639870695852"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.19209217238223933"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
